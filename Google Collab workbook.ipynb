{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 6 - Data Ingestion Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z8ao9Zt_dQjq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inH4Y8UwyUCN"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHiqWMUClZMP"
      },
      "source": [
        "!pip install modin[ray]\n",
        "!pip install 'ray[default]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnqDQTlDhXaC"
      },
      "source": [
        "!pip install \"dask[dataframe]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-SfKZph5St"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('./parking_violations_issued_fiscal_year_2016.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4QOJV2QyIsz",
        "outputId": "321c7d74-150d-4826-89e0-fe84c76007f1"
      },
      "source": [
        "%%writefile configuration.yaml\n",
        "file_type: csv\n",
        "dataset_name: parking_violations_issued\n",
        "file_name: parking_violations_issued_fiscal_year_2016\n",
        "table_name: parking_violations\n",
        "inbound_delimiter: \",\"\n",
        "outbound_delimiter: \"|\"\n",
        "skip_leading_rows: 1\n",
        "columns:\n",
        "    - plate_type\n",
        "    - vehicle_body_type\n",
        "    - violation_code\n",
        "    - registration_state\n",
        "    - vehicle_make\n",
        "    - issue_date\n",
        "    - summons_number\n",
        "    - issuing_agency\n",
        "    - street_code1\n",
        "    - plate_id\n",
        "    - violation_post_code\n",
        "    - bbl\n",
        "    - street_code2\n",
        "    - street_code3\n",
        "    - meter_number\n",
        "    - double_parking_violation\n",
        "    - violation_time\n",
        "    - nta\n",
        "    - community_council\n",
        "    - date_first_observed\n",
        "    - feet_from_curb\n",
        "    - bin\n",
        "    - street_name\n",
        "    - issuer_code\n",
        "    - issuer_precinct\n",
        "    - intersecting_street\n",
        "    - vehicle_year\n",
        "    - time_first_observed\n",
        "    - no_standing_or_stopping_violation\n",
        "    - law_section\n",
        "    - issuer_command\n",
        "    - violation_in_front_of_or_opposite\n",
        "    - violation_location\n",
        "    - days_parking_in_effect\n",
        "    - longitude\n",
        "    - unregistered_vehicle\n",
        "    - violation_description\n",
        "    - latitude\n",
        "    - violation_precinct\n",
        "    - from_hours_in_effect\n",
        "    - house_number\n",
        "    - violation_legal_code\n",
        "    - census_tract\n",
        "    - vehicle_expiration_date\n",
        "    - issuer_squad\n",
        "    - violation_county\n",
        "    - sub_division\n",
        "    - community_board\n",
        "    - hydrant_violation\n",
        "    - vehicle_color\n",
        "    - to_hours_in_effect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configuration.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-ZRgs3O0EiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qD_UPdYe0wM"
      },
      "source": [
        "# Read Large File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OS8c09be_6D",
        "outputId": "5351f55f-2264-4912-b51d-de96821b3866"
      },
      "source": [
        "import dask.dataframe\n",
        "import modin.pandas as m_ray_pd # using the Ray core in modin\n",
        "import pandas as pd\n",
        "import time\n",
        "import ray\n",
        "\n",
        "input_file = 'parking_violations_issued_fiscal_year_2016.csv' # 6+ GB file\n",
        "\n",
        "# Pandas - (runs out of RAM and crashes Google Colab)\n",
        "start_time = time.time();\n",
        "data = pd.read_csv(input_file);\n",
        "print('Pandas took %s seconds' % (time.time() - start_time))\n",
        "\n",
        "# Pandas[chunksize] - (runs out of RAM and crashes Google Colab)\n",
        "start_time = time.time();\n",
        "data = pd.read_csv(input_file, chunksize=100000);\n",
        "print('Pandas took with chunksize %s seconds' % (time.time() - start_time))\n",
        "\n",
        "# Modin[Ray] \n",
        "start_time = time.time();\n",
        "data = m_ray_pd.read_csv(input_file);\n",
        "print('Modin[Ray] %s seconds' % (time.time() - start_time))\n",
        "\n",
        "# dask \n",
        "start_time = time.time();\n",
        "data = dask.dataframe.read_csv(input_file);\n",
        "print('Dask took %s seconds' % (time.time() - start_time))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DtypeWarning: Columns (17,18,20,21,22,23,29,30,31,32,34,36,38,39) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pandas took 69.10192894935608 seconds\n",
            "Pandas took with chunksize 1.3400604724884033 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "UserWarning: Ray execution environment not yet initialized. Initializing...\n",
            "To remove this warning, run the following python code before doing dataframe operations:\n",
            "\n",
            "    import ray\n",
            "    ray.init()\n",
            "\n",
            "2021-07-12 01:10:30,902\tWARNING services.py:1740 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 6308233216 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=8.20gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
            "\u001b[2m\u001b[36m(pid=4811)\u001b[0m tcmalloc: large alloc 1075970048 bytes == 0x55ce2ac20000 @  0x7f3f4756a1e7 0x55ce26f94e68 0x55ce26f5f637 0x55ce27040a6e 0x55ce26f62b59 0x55ce27053fed 0x55ce26fd6988 0x55ce26fd14ae 0x55ce26ea3e2c 0x55ce26fd3bb5 0x55ce26ea3d14 0x55ce26fd3bb5 0x55ce26fd17ad 0x55ce26ea3eb1 0x7f3f4432821f 0x7f3f443cede9 0x7f3f4433019a 0x7f3f444a0a3f 0x7f3f444154e2 0x7f3f444e301d 0x7f3f444e39ca 0x7f3f44446472 0x7f3f4486b238 0x7f3f44968bf1 0x7f3f44968d21 0x7f3f4496a820 0x7f3f444aa950 0x7f3f4432b0a7 0x55ce26f62bb1 0x55ce27053fed 0x55ce26fd6988\n",
            "\u001b[2m\u001b[36m(pid=4810)\u001b[0m tcmalloc: large alloc 1075970048 bytes == 0x55e36d614000 @  0x7f026756f1e7 0x55e36a9d2e68 0x55e36a99d637 0x55e36aa7ea6e 0x55e36a9a0b59 0x55e36aa91fed 0x55e36aa14988 0x55e36aa0f4ae 0x55e36a8e1e2c 0x55e36aa11bb5 0x55e36a8e1d14 0x55e36aa11bb5 0x55e36aa0f7ad 0x55e36a8e1eb1 0x7f026432d21f 0x7f02643d3de9 0x7f026433519a 0x7f02644a5a3f 0x7f026441a4e2 0x7f02644e801d 0x7f02644e89ca 0x7f026444b472 0x7f0264870238 0x7f026496dbf1 0x7f026496dd21 0x7f026496f820 0x7f02644af950 0x7f02643300a7 0x55e36a9a0bb1 0x55e36aa91fed 0x55e36aa14988\n",
            "2021-07-12 01:12:11,145\tWARNING worker.py:1123 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Task ID: a67dc375e60ddd1affffffffffffffffffffffff01000000 Worker ID: 15ee16ed3dce6cb7058ed81d346c191c15e0e92b3cc24a339e1d899f Node ID: 7c9fcd7868247f54d100856f74b7eee4110da55e266876f9c086527e Worker IP address: 172.28.0.2 Worker port: 42967 Worker PID: 4810\n",
            "\u001b[2m\u001b[36m(pid=4894)\u001b[0m tcmalloc: large alloc 1075970048 bytes == 0x55dfc7152000 @  0x7f75b7b581e7 0x55dfc3ff0e68 0x55dfc3fbb637 0x55dfc409ca6e 0x55dfc3fbeb59 0x55dfc40affed 0x55dfc4032988 0x55dfc402d4ae 0x55dfc3effe2c 0x55dfc402fbb5 0x55dfc3effd14 0x55dfc402fbb5 0x55dfc402d7ad 0x55dfc3effeb1 0x7f75b491621f 0x7f75b49bcde9 0x7f75b491e19a 0x7f75b4a8ea3f 0x7f75b4a034e2 0x7f75b4ad101d 0x7f75b4ad19ca 0x7f75b4a34472 0x7f75b4e59238 0x7f75b4f56bf1 0x7f75b4f56d21 0x7f75b4f58820 0x7f75b4a98950 0x7f75b49190a7 0x55dfc3fbebb1 0x55dfc40affed 0x55dfc4032988\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Modin[Ray] 193.85107588768005 seconds\n",
            "Dask took 0.1345052719116211 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8ao9Zt_dQjq"
      },
      "source": [
        "# Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3rDHb0XGNMc",
        "outputId": "b03ec11a-15a4-4dda-88f1-06e053e2efef"
      },
      "source": [
        "%%writefile utility.py\n",
        "import math\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import datetime \n",
        "import gc\n",
        "import re\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "\n",
        "################\n",
        "# File Reading #\n",
        "################\n",
        "\n",
        "def read_config_file(filepath):\n",
        "    with open(filepath, 'r') as stream:\n",
        "        try:\n",
        "            return yaml.safe_load(stream)\n",
        "        except yaml.YAMLError as exc:\n",
        "            logging.error(exc)\n",
        "\n",
        "\n",
        "def replacer(string, char):\n",
        "    pattern = char + '{2,}'\n",
        "    string = re.sub(pattern, char, string) \n",
        "    return string\n",
        "\n",
        "def col_header_val(df,table_config):\n",
        "    '''\n",
        "    replace whitespaces in the column\n",
        "    and standardized column names\n",
        "    '''\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
        "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
        "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
        "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
        "    expected_col.sort()\n",
        "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
        "    df = df.reindex(sorted(df.columns), axis=1)\n",
        "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"column name and column length validation failed\")\n",
        "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
        "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
        "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
        "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
        "        logging.info(f'df columns: {df.columns}')\n",
        "        logging.info(f'expected columns: {expected_col}')\n",
        "        return 0\n",
        "\n",
        "def human_size(nbytes):\n",
        "  suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
        "  human = nbytes\n",
        "  rank = 0\n",
        "  if nbytes != 0:\n",
        "    rank = int((math.log10(nbytes)) / 3)\n",
        "    rank = min(rank, len(suffixes) - 1)\n",
        "    human = nbytes / (1024.0 ** rank)\n",
        "  f = ('%.2f' % human).rstrip('0').rstrip('.')\n",
        "  return '%s %s' % (f, suffixes[rank])\n",
        "\n",
        "def file_summary(df,table_config):\n",
        "    # get file size and convert bytes to readable string\n",
        "    file_type = table_config['file_type']\n",
        "    file_name = table_config['file_name'] + f'.{file_type}'\n",
        "\n",
        "    THIS_FOLDER = os.path.dirname(os.path.abspath(__file__))\n",
        "    file_full_path = os.path.join(THIS_FOLDER, file_name)\n",
        "\n",
        "    file_size = os.path.getsize(file_full_path)\n",
        "    size_readable = human_size(file_size)\n",
        "\n",
        "    # get number of columns\n",
        "    number_of_cols = df.shape[1]\n",
        "\n",
        "    # get number of rows\n",
        "    number_of_rows = df.shape[0]\n",
        "\n",
        "    # print file summary\n",
        "    print('FILE SUMMARY FOR: ', file_name)\n",
        "    print('Total number of rows: ', number_of_rows)\n",
        "    print('Total number of columns: ', number_of_cols)\n",
        "    print('File size: ', size_readable)\n",
        "\n",
        "def saveFile(df,table_config):\n",
        "    # save dataframe to text file seperated by |\n",
        "    df.to_csv(r'./saved_data.txt', header=None, index=None, sep=table_config['outbound_delimiter'], mode='a')\n",
        "\n",
        "    # comppress saved text file to gz format\n",
        "    with open('./saved_data.txt', 'rb') as f_in, gzip.open(table_config['file_name'] + '.txt' + '.gz', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utility.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7pfYasHPGz"
      },
      "source": [
        "# Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te15Io-iHSPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cac3c00-8057-48b5-b876-659c4e2e9abd"
      },
      "source": [
        "import pandas as pd\n",
        "import utility as util\n",
        "\n",
        "# Read config file\n",
        "config_data = util.read_config_file(\"configuration.yaml\")\n",
        "\n",
        "# read the file using config file\n",
        "file_type = config_data['file_type']\n",
        "source_file = \"./\" + config_data['file_name'] + f'.{file_type}'\n",
        "#print(\"\",source_file)\n",
        "df = pd.read_csv(source_file,config_data['inbound_delimiter'],)\n",
        "\n",
        "\n",
        "if util.col_header_val(df,config_data)==0:\n",
        "    print(\"validation failed\")\n",
        "    # write code to reject the file\n",
        "else:\n",
        "    print(\"col validation passed\")\n",
        "    # write the code to perform further action\n",
        "    # in the pipleine\n",
        "    util.file_summary(df,config_data)\n",
        "    util.saveFile(df,config_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (17,18,20,21,22,23,29,30,31,32,34,36,38,39) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "col validation passed\n",
            "FILE SUMMARY FOR:  parking_violations_issued_fiscal_year_2016.csv\n",
            "Total number of rows:  10626899\n",
            "Total number of columns:  51\n",
            "File size:  2 GB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}